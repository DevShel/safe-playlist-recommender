{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# User Analysis\n",
        "\n",
        "As part of my spotify playlist recommendation project, I will be now engineering user data. I am starting with around 1m rows of (song_id, user_id, num_listens). I also have the \"safety scores\" of each of these songs, which is an object that looks includes these values, with each being a float from 0-1:\n",
        "\n",
        "```\n",
        "threshold_sexual, threshold_harassment, threshold_harassment_threatening,\tthreshold_hate,\tthreshold_hate_threatening,\tthreshold_illicit,\tthreshold_illicit_violent,\tthreshold_violence,\tthreshold_violence_graphic\n",
        "```\n",
        "\n",
        "My goal is to determine each user's 85% for each category. This will tell me what they're comfortable listening to, and better inform my safe recommendation system."
      ],
      "metadata": {
        "id": "_K9H9xb-aMAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "triplets_data = pd.read_csv('triplets_cleaned.csv')\n",
        "\n",
        "songs_per_user = triplets_data.groupby('user_id').size()\n",
        "\n",
        "total_users = len(songs_per_user)\n",
        "mean_songs = songs_per_user.mean()\n",
        "median_songs = songs_per_user.median()\n",
        "std_songs = songs_per_user.std()\n",
        "\n",
        "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
        "percentile_values = [songs_per_user.quantile(p/100) for p in percentiles]\n",
        "\n",
        "print(f\"Total users: {total_users:,}\")\n",
        "print(f\"Mean songs per user: {mean_songs:.1f}\")\n",
        "print(f\"Median songs per user: {median_songs:.1f}\")\n",
        "print(f\"Standard deviation: {std_songs:.1f}\")\n",
        "print()\n",
        "\n",
        "for p, val in zip(percentiles, percentile_values):\n",
        "    print(f\"{p}th percentile: {val:.0f} songs\")\n",
        "\n",
        "print()\n",
        "\n",
        "cutoff_options = [5, 10, 15, 20, 25, 30]\n",
        "for cutoff in cutoff_options:\n",
        "    users_below_cutoff = (songs_per_user < cutoff).sum()\n",
        "    percentage_below = (users_below_cutoff / total_users) * 100\n",
        "    print(f\"Users with <{cutoff} songs: {users_below_cutoff:,} ({percentage_below:.1f}%)\")\n",
        "\n",
        "recommended_cutoff = max(10, min(30, int(mean_songs * 0.3)))\n",
        "print(f\"\\nRecommended cutoff based on data: {recommended_cutoff} songs\")\n",
        "print(f\"This captures users with <{(songs_per_user < recommended_cutoff).sum() / total_users * 100:.1f}% of listening data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9uefj_u-6FR",
        "outputId": "36349611-33bd-4e7e-dffc-cf5c8f59ba99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total users: 73,949\n",
            "Mean songs per user: 13.4\n",
            "Median songs per user: 8.0\n",
            "Standard deviation: 17.8\n",
            "\n",
            "10th percentile: 2 songs\n",
            "25th percentile: 4 songs\n",
            "50th percentile: 8 songs\n",
            "75th percentile: 16 songs\n",
            "90th percentile: 31 songs\n",
            "95th percentile: 45 songs\n",
            "99th percentile: 88 songs\n",
            "\n",
            "Users with <5 songs: 22,241 (30.1%)\n",
            "Users with <10 songs: 43,141 (58.3%)\n",
            "Users with <15 songs: 53,562 (72.4%)\n",
            "Users with <20 songs: 59,634 (80.6%)\n",
            "Users with <25 songs: 63,479 (85.8%)\n",
            "Users with <30 songs: 66,041 (89.3%)\n",
            "\n",
            "Recommended cutoff based on data: 10 songs\n",
            "This captures users with <58.3% of listening data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataframe"
      ],
      "metadata": {
        "id": "SJW_iJ4R9mpw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "B1UHcbpN60Dc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "triplets_data = pd.read_csv('triplets_cleaned.csv')\n",
        "all_user_ids = triplets_data['user_id'].unique()\n",
        "\n",
        "safety_category_names = ['sexual', 'harassment', 'harassment_threatening', 'hate',\n",
        "                        'hate_threatening', 'illicit', 'illicit_violent', 'violence', 'violence_graphic']\n",
        "\n",
        "threshold_column_names = [f'threshold_{category}' for category in safety_category_names]\n",
        "\n",
        "initial_dataframe = pd.DataFrame({\n",
        "    'user_id': all_user_ids,\n",
        "    'song_count': np.nan,\n",
        "    'threshold_method': '',\n",
        "    **{col: np.nan for col in threshold_column_names}\n",
        "})\n",
        "\n",
        "initial_dataframe.to_csv('user_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Threshold of Each Category\n",
        "Calculating the weighted 85% percentile threshold for each of the respective categories. This algorithm may seem complicated, but it's just:\n",
        "\n",
        "\n",
        "* **Pair and sort**\n",
        "  Places songs in increasing safety score order, carrying along each song's listen-count weight.\n",
        "* **Compute cumulative weight**\n",
        "  It running-totals those listen counts so you know, at each score, what fraction of all listens falls at or below that score.\n",
        "* **Determine the 85% cutoff**\n",
        "  It multiplies user's total listen-count by 0.85 to find the “target weight” where 85% of plays have lower or equal safety scores.\n",
        "* **Locate the breakpoint**\n",
        "  It finds the first index in the cumulative weights that meets or exceeds that target weight.\n",
        "* **Return the score**\n",
        "  The safety score at that index is your weighted 85th percentile.\n"
      ],
      "metadata": {
        "id": "ePY5agDI9rdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def calculate_weighted_percentile(score_values, listen_count_weights, target_percentile):\n",
        "    # Sort safety scores from lowest to highest\n",
        "    sorted_indices = np.argsort(score_values)\n",
        "    sorted_scores = score_values[sorted_indices]\n",
        "    sorted_weights = listen_count_weights[sorted_indices]\n",
        "\n",
        "    # Calculate cumulative weights, weighting the vategory value by how many listens the song has\n",
        "    cumulative_weights = np.cumsum(sorted_weights)\n",
        "    total_weight = cumulative_weights[-1]\n",
        "\n",
        "    # Find the target weight position (e.g., 85% of total weight)\n",
        "    target_weight = target_percentile / 100.0 * total_weight\n",
        "    insertion_index = np.searchsorted(cumulative_weights, target_weight)\n",
        "\n",
        "    return sorted_scores[insertion_index]"
      ],
      "metadata": {
        "id": "iUmfDE1ga0Ub"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "existing_thresholds_df = pd.read_csv('user_data.csv')\n",
        "triplets_listening_data = pd.read_csv('triplets_cleaned.csv')\n",
        "songs_with_safety_scores = pd.read_csv('joined_songs_cleaned.csv')\n",
        "songs_with_safety_scores['parsed_safety_scores'] = songs_with_safety_scores['safety_scores'].apply(json.loads)\n",
        "safety_category_list = ['sexual', 'harassment', 'harassment/threatening', 'hate',\n",
        "                       'hate/threatening', 'illicit', 'illicit/violent', 'violence', 'violence/graphic']\n",
        "\n",
        "for safety_category in safety_category_list:\n",
        "    column_name = f'safety_{safety_category.replace(\"/\", \"_\")}'\n",
        "    songs_with_safety_scores[column_name] = songs_with_safety_scores['parsed_safety_scores'].apply(lambda x: x[safety_category])\n",
        "\n",
        "merged_user_song_data = triplets_listening_data.merge(songs_with_safety_scores, on='song_id', how='inner')\n",
        "\n",
        "minimum_songs_for_personalization = 5\n",
        "user_percentile_threshold = 85\n",
        "\n",
        "conservative_default_thresholds = {}\n",
        "for safety_category in safety_category_list:\n",
        "    safety_column_name = f'safety_{safety_category.replace(\"/\", \"_\")}'\n",
        "    conservative_default_thresholds[safety_category] = np.percentile(songs_with_safety_scores[safety_column_name], 95)\n",
        "\n",
        "users_needing_processing = existing_thresholds_df[existing_thresholds_df['song_count'].isna()]\n",
        "\n",
        "for user_index, user_row in users_needing_processing.iterrows():\n",
        "    current_user_id = user_row['user_id']\n",
        "    current_user_songs = merged_user_song_data[merged_user_song_data['user_id'] == current_user_id]\n",
        "\n",
        "    user_song_count = len(current_user_songs)\n",
        "    existing_thresholds_df.loc[user_index, 'song_count'] = user_song_count\n",
        "\n",
        "    if user_song_count < minimum_songs_for_personalization:\n",
        "        for safety_category in safety_category_list:\n",
        "            threshold_column = f'threshold_{safety_category.replace(\"/\", \"_\")}'\n",
        "            existing_thresholds_df.loc[user_index, threshold_column] = conservative_default_thresholds[safety_category]\n",
        "        existing_thresholds_df.loc[user_index, 'threshold_method'] = 'conservative_default'\n",
        "    else:\n",
        "        for safety_category in safety_category_list:\n",
        "            safety_column_name = f'safety_{safety_category.replace(\"/\", \"_\")}'\n",
        "            user_safety_scores = current_user_songs[safety_column_name].values\n",
        "            user_listen_counts = current_user_songs['listen_count'].values\n",
        "\n",
        "            personalized_threshold = calculate_weighted_percentile(user_safety_scores, user_listen_counts, user_percentile_threshold)\n",
        "            threshold_column = f'threshold_{safety_category.replace(\"/\", \"_\")}'\n",
        "            existing_thresholds_df.loc[user_index, threshold_column] = personalized_threshold\n",
        "\n",
        "        existing_thresholds_df.loc[user_index, 'threshold_method'] = f'weighted_{user_percentile_threshold}th_percentile'\n",
        "    if user_index % 1000 == 0:\n",
        "        existing_thresholds_df.to_csv('user_data.csv', index=False)\n",
        "        print(f\"Processed {user_index} users\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ncVHI2_9sDC",
        "outputId": "562e5b50-b5da-425a-9737-8c451a9261ee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3-2950821678.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  existing_thresholds_df = pd.read_csv('user_data.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 48000 users\n",
            "Processed 49000 users\n",
            "Processed 50000 users\n",
            "Processed 51000 users\n",
            "Processed 52000 users\n",
            "Processed 53000 users\n",
            "Processed 54000 users\n",
            "Processed 55000 users\n",
            "Processed 56000 users\n",
            "Processed 57000 users\n",
            "Processed 58000 users\n",
            "Processed 59000 users\n",
            "Processed 60000 users\n",
            "Processed 61000 users\n",
            "Processed 62000 users\n",
            "Processed 63000 users\n",
            "Processed 64000 users\n",
            "Processed 65000 users\n",
            "Processed 66000 users\n",
            "Processed 67000 users\n",
            "Processed 68000 users\n",
            "Processed 69000 users\n",
            "Processed 70000 users\n",
            "Processed 71000 users\n",
            "Processed 72000 users\n",
            "Processed 73000 users\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate New Thresholds\n",
        "We previously gave users with less than 5 songs, song-based conservative thresholds. This is to prevent problematic cold starting.\n",
        "\n",
        "Now that we have user data, lets give users user-based thresholds, for a more realistic interpretation of thresholds.\n",
        "\n",
        "We will just find the 85th percentile of user's listening thresholds per category, across all users that have 5 or more songs. This isn't weighted, like before."
      ],
      "metadata": {
        "id": "ryFrL4l7U78_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "user_data = pd.read_csv('user_data.csv')\n",
        "\n",
        "# Remove users with blank or 0 song listens, forgot to do before\n",
        "initial_count = len(user_data)\n",
        "user_data = user_data.dropna(subset=['song_count'])\n",
        "user_data = user_data[user_data['song_count'] > 0]\n",
        "removed_count = initial_count - len(user_data)\n",
        "\n",
        "safety_categories = ['sexual', 'harassment', 'harassment_threatening', 'hate',\n",
        "                    'hate_threatening', 'illicit', 'illicit_violent', 'violence', 'violence_graphic']\n",
        "\n",
        "personalized_users = user_data[user_data['threshold_method'] == 'weighted_85th_percentile']\n",
        "\n",
        "new_conservative_defaults = {}\n",
        "for category in safety_categories:\n",
        "    col = f'threshold_{category}'\n",
        "    new_conservative_defaults[col] = np.percentile(personalized_users[col], 85)\n",
        "\n",
        "conservative_mask = user_data['threshold_method'] == 'conservative_default'\n",
        "for category in safety_categories:\n",
        "    col = f'threshold_{category}'\n",
        "    user_data.loc[conservative_mask, col] = new_conservative_defaults[col]\n",
        "\n",
        "user_data.to_csv('user_data_updated.csv', index=False)\n",
        "\n",
        "print(f\"Removed {removed_count} users with blank or 0 song listens\")\n",
        "print(f\"Updated {conservative_mask.sum()} users with new conservative defaults\")\n",
        "print(f\"Total users in final dataset: {len(user_data)}\")\n",
        "print(\"\\nNew conservative thresholds (85th percentile of user thresholds):\")\n",
        "for category in safety_categories:\n",
        "    print(f\"{category}: {new_conservative_defaults[f'threshold_{category}']:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzyyuwbdVCwH",
        "outputId": "140c5fa4-a5ee-45e2-94bf-c803bdba7cae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 10893 users with blank or 0 song listens\n",
            "Updated 19050 users with new conservative defaults\n",
            "Total users in final dataset: 63056\n",
            "\n",
            "New conservative thresholds (85th percentile of user thresholds):\n",
            "sexual: 0.204482\n",
            "harassment: 0.197983\n",
            "harassment_threatening: 0.019397\n",
            "hate: 0.008030\n",
            "hate_threatening: 0.003347\n",
            "illicit: 0.006253\n",
            "illicit_violent: 0.000095\n",
            "violence: 0.403610\n",
            "violence_graphic: 0.016081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "songs = pd.read_csv('joined_songs_cleaned.csv')\n",
        "songs['parsed_scores'] = songs['safety_scores'].apply(json.loads)\n",
        "\n",
        "thresholds = {\n",
        "    'sexual': 0.204482,\n",
        "    'harassment': 0.197983,\n",
        "    'harassment/threatening': 0.019397,\n",
        "    'hate': 0.008030,\n",
        "    'hate/threatening': 0.003347,\n",
        "    'illicit': 0.006253,\n",
        "    'illicit/violent': 0.000095,\n",
        "    'violence': 0.403610,\n",
        "    'violence/graphic': 0.016081\n",
        "}\n",
        "\n",
        "print(\"Songs exceeding thresholds:\")\n",
        "for category, threshold in thresholds.items():\n",
        "    exceeds = sum(songs['parsed_scores'].apply(lambda x: x[category] > threshold))\n",
        "    pct = (exceeds / len(songs)) * 100\n",
        "    print(f\"{category}: {exceeds} songs ({pct:.1f}%)\")\n",
        "\n",
        "print(\"\\nThreshold percentiles in song distribution:\")\n",
        "for category, threshold in thresholds.items():\n",
        "    scores = songs['parsed_scores'].apply(lambda x: x[category])\n",
        "    percentile = (scores <= threshold).mean() * 100\n",
        "    print(f\"{category}: {percentile:.1f}th percentile\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqopLprbVlQZ",
        "outputId": "52ea4022-cdf3-4bf1-d2bf-b71ea095895b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Songs exceeding thresholds:\n",
            "sexual: 289 songs (6.0%)\n",
            "harassment: 362 songs (7.5%)\n",
            "harassment/threatening: 379 songs (7.9%)\n",
            "hate: 343 songs (7.1%)\n",
            "hate/threatening: 352 songs (7.3%)\n",
            "illicit: 373 songs (7.7%)\n",
            "illicit/violent: 381 songs (7.9%)\n",
            "violence: 362 songs (7.5%)\n",
            "violence/graphic: 322 songs (6.7%)\n",
            "\n",
            "Threshold percentiles in song distribution:\n",
            "sexual: 94.0th percentile\n",
            "harassment: 92.5th percentile\n",
            "harassment/threatening: 92.1th percentile\n",
            "hate: 92.9th percentile\n",
            "hate/threatening: 92.7th percentile\n",
            "illicit: 92.3th percentile\n",
            "illicit/violent: 92.1th percentile\n",
            "violence: 92.5th percentile\n",
            "violence/graphic: 93.3th percentile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Average Number of Plays Per Song, Per User"
      ],
      "metadata": {
        "id": "tmfhBGFMHzG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Loading triplets data...\")\n",
        "triplets_data = pd.read_csv('triplets_cleaned.csv')\n",
        "\n",
        "print(\"Calculating average plays per song for each user...\")\n",
        "user_avg_plays = triplets_data.groupby('user_id')['listen_count'].mean().reset_index()\n",
        "user_avg_plays.columns = ['user_id', 'avg_plays_per_song']\n",
        "\n",
        "print(\"Loading existing user data...\")\n",
        "user_data = pd.read_csv('user_data_updated.csv')\n",
        "initial_columns = user_data.columns.tolist()\n",
        "\n",
        "print(\"Merging average plays data...\")\n",
        "user_data = user_data.merge(user_avg_plays, on='user_id', how='left')\n",
        "\n",
        "user_data.to_csv('user_data_updated.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdY8eQeJHv5R",
        "outputId": "63885225-7a0b-49fe-de51-916563698aba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading triplets data...\n",
            "Calculating average plays per song for each user...\n",
            "Loading existing user data...\n",
            "Merging average plays data...\n"
          ]
        }
      ]
    }
  ]
}